{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2196f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:27:52.538676Z",
     "start_time": "2023-05-27T04:27:52.143896Z"
    },
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-07T02:13:52.026534Z",
     "iopub.status.busy": "2025-06-07T02:13:52.026310Z",
     "iopub.status.idle": "2025-06-07T02:13:52.583442Z",
     "shell.execute_reply": "2025-06-07T02:13:52.581816Z",
     "shell.execute_reply.started": "2025-06-07T02:13:52.026511Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "259deb1bf2a9d14ab70ce8837331036b",
     "grade": false,
     "grade_id": "cell-402d2262fbb79bd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from numpy.testing import assert_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e78bd-b1e8-4ee5-862e-2c2946b12e27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fea2e844f4d62292df3173930caac41",
     "grade": false,
     "grade_id": "cell-50ea938eeef28799",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "For this problem, you will be working with the [Humor Detection from Product Question Answering Systems](https://registry.opendata.aws/humor-detection/) data set.\n",
    "\n",
    "You will perform the tasks for this problem on Jojie and you will answer them directly on this notebook. You must work directly with the data on S3. Do **not** download. You may **only** use Apache Spark and the Python Standard Library. You **cannot** use numpy, scipy, pandas or scikit-learn. Do **not** print or display large amount of results. You will get deductions if you make the browser unresponsive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30923a-e999-40c3-ad24-787a87f3f294",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0289954fee6a1b6be9afbda4aa6b904",
     "grade": false,
     "grade_id": "cell-f459ccd55684dd03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 2a [5 pts]\n",
    "\n",
    "Create a function `compute_pcc` that will return the PCC for `label`. Exclude null `label`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e05569-f835-4b3e-aecc-701388c97684",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-06-07T07:19:29.818793Z",
     "iopub.status.busy": "2025-06-07T07:19:29.818106Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c51acb5ec86b09910c89c7f6d4047c5",
     "grade": false,
     "grade_id": "cell-bc98d502308bce14",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BDCCFinalExam\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-aws:3.2.0,\" \n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.11.375\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b07ff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 16:54:50 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- question: string (nullable = true)\n",
      " |-- product_description: string (nullable = true)\n",
      " |-- image_url: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "s3_path = \"s3a://humor-detection-pds/*.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(s3_path)\n",
    "df = df.filter(col(\"label\").isNotNull())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ffd000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "label",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "273dac10-a776-4cdf-9550-f607ece0222f",
       "rows": [
        [
         "0",
         "19138"
        ],
        [
         "1",
         "9567"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "label\n",
       "0    19138\n",
       "1     9567\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b798560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pcc():\n",
    "    class_counts = (\n",
    "            df.groupBy(\"label\")\n",
    "              .agg(count(\"*\").alias(\"count\"))\n",
    "              .toPandas()\n",
    "        )\n",
    "    class_counts[\"percentage\"] = class_counts[\"count\"] / class_counts[\"count\"].sum()\n",
    "    pcc = sum(class_counts[\"percentage\"] ** 2)\n",
    "    return pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a665e5f4-9510-4b7f-9ee5-ab692e4c99e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-07T02:13:52.595529Z",
     "iopub.status.busy": "2025-06-07T02:13:52.595196Z",
     "iopub.status.idle": "2025-06-07T02:14:55.979448Z",
     "shell.execute_reply": "2025-06-07T02:14:55.977720Z",
     "shell.execute_reply.started": "2025-06-07T02:13:52.595499Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7025f7fdc2c026640d093eb0deb05d7",
     "grade": true,
     "grade_id": "cell-d182b1905e6f7b41",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "pcc = compute_pcc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba797d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportional Chance Criterion (PCC): 0.5556\n"
     ]
    }
   ],
   "source": [
    "print(f\"Proportional Chance Criterion (PCC): {pcc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b010d-faf3-4405-85a9-be03a4397d25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb9b65bf6306e343d33f5daeb0758568",
     "grade": false,
     "grade_id": "cell-b28adacea31d7c77",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 2b [15 pts]\n",
    "\n",
    "Using `question` as a feature, create a trained MLlib supervised ML model for predicting `label` that uses hashing as a preprocessing step. Test accuracy should be at least 1.25 * PCC. Make sure that the steps are clearly documented and the value of test accuracy is explicitly stated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "249ed10c-f182-447a-b1a3-757b69896909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 16:55:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "[Stage 30:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7776\n",
      "PCC: 0.5556\n",
      "1.25 * PCC: 0.6945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "# Step 1: Tokenize the question\n",
    "tokenizer = Tokenizer(inputCol=\"question\", outputCol=\"tokens\")\n",
    "\n",
    "# Step 2: HashingTF to convert tokens into numerical vectors\n",
    "hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"features\", numFeatures=10000)\n",
    "\n",
    "# Step 3: Train a logistic regression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "\n",
    "# Step 4: Assemble pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashing_tf, lr])\n",
    "\n",
    "# Step 5: Train/Test split\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 6: Fit the model\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Step 7: Predict\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Step 8: Evaluate Accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "test_accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Step 9: Compute PCC\n",
    "from numpy.testing import assert_almost_equal\n",
    "pcc = compute_pcc()\n",
    "\n",
    "# Step 10: Print and assert accuracy threshold\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"PCC: {pcc:.4f}\")\n",
    "print(f\"1.25 * PCC: {1.25 * pcc:.4f}\")\n",
    "\n",
    "assert test_accuracy >= 1.25 * pcc, \"Accuracy is below the required 1.25 * PCC threshold.\""
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "bdcc-final-exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
