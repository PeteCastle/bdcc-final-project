{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051fd701",
   "metadata": {},
   "source": [
    "### Installing\n",
    "\n",
    "Ensure that you have Homebrew installed on your macOS system.\n",
    "\n",
    "```bash\n",
    "brew install openjdk@17\n",
    "brew link --force --overwrite openjdk@17\n",
    "```\n",
    "\n",
    "Modify your shell configuration file (e.g., `~/.bash_profile`, `~/.zshrc`, etc.) to set the environment variables:\n",
    "```bash\n",
    "export JAVA_HOME=$(/usr/libexec/java_home -v 17)\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "Install AWS dependencies:\n",
    "```bash\n",
    "pyspark --packages org.apache.hadoop:hadoop-aws:3.3.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22486d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "from sedona.spark import SedonaContext\n",
    "import os\n",
    "from pyspark.sql.functions import col, explode\n",
    "from datetime import datetime, date\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType, MapType\n",
    "from tqdm import tqdm\n",
    "from sedona.sql.types import GeometryType\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600c8c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/30 09:56:50 WARN Utils: Your hostname, PeteCastle.local resolves to a loopback address: 127.0.0.1; using 192.168.45.216 instead (on interface en0)\n",
      "25/05/30 09:56:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.4\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.18, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_451\n",
      "Branch HEAD\n",
      "Compiled by user yangjie01 on 2024-12-17T04:51:46Z\n",
      "Revision a6f220d951742f4074b37772485ee0ec7a774e7d\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0211d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/30 09:56:51 WARN Utils: Your hostname, PeteCastle.local resolves to a loopback address: 127.0.0.1; using 192.168.45.216 instead (on interface en0)\n",
      "25/05/30 09:56:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/francismarkcayco/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/francismarkcayco/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-shaded-3.3_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7d02c370-b66b-4f32-bff6-34bc1f6441ff;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-spark-shaded-3.3_2.12;1.7.1 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/bdcc-final-exam/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.7.1-28.5 in central\n",
      ":: resolution report :: resolve 106ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-shaded-3.3_2.12;1.7.1 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.7.1-28.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7d02c370-b66b-4f32-bff6-34bc1f6441ff\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/3ms)\n",
      "25/05/30 09:56:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sedona = SedonaContext.builder() \\\n",
    "    .appName(\"BDCCFinalExam\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.storage.memoryFraction\",\"0.4\") \\\n",
    "    .config(\"spark.memory.fraction\",\"0.6\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.1,'\n",
    "            \"org.apache.hadoop:hadoop-aws:3.2.0,\"\\\n",
    "            'org.datasyslab:geotools-wrapper:1.7.1-28.5,'\\\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.11.375\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e14865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/qqxkptrd1bg66t_3104htv1m0000gn/T/ipykernel_65963/3629718819.py:6: DeprecationWarning: Call to deprecated function registerAll (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  SedonaRegistrator.registerAll(sedona)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sedona.register import SedonaRegistrator\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "\n",
    "# Register Sedona UDTs and functions\n",
    "SedonaRegistrator.registerAll(sedona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8b1b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot dates:\n",
      "2025-01-01\n",
      "2025-04-01\n",
      "2024-01-01\n",
      "2024-04-01\n",
      "2024-07-01\n",
      "2024-10-01\n",
      "2023-01-01\n",
      "2023-04-01\n",
      "2023-07-01\n",
      "2023-10-01\n",
      "2022-01-01\n",
      "2022-04-01\n",
      "2022-07-01\n",
      "2022-10-01\n",
      "2021-01-01\n",
      "2021-04-01\n",
      "2021-07-01\n",
      "2021-10-01\n",
      "2020-01-01\n",
      "2020-04-01\n",
      "2020-07-01\n",
      "2020-10-01\n",
      "2019-01-01\n",
      "2019-04-01\n",
      "2019-07-01\n",
      "2019-10-01\n",
      "2018-01-01\n",
      "2018-04-01\n",
      "2018-07-01\n",
      "2018-10-01\n"
     ]
    }
   ],
   "source": [
    "snapshot_dates = [\n",
    "    date(year, month, 1)\n",
    "    for year in range(2025, 2017, -1)\n",
    "    for month in (1, 4, 7, 10)\n",
    "]\n",
    "\n",
    "# filter out snapshot dates that are in future\n",
    "snapshot_dates = [d for d in snapshot_dates if d <= date.today()]\n",
    "\n",
    "print(\"Snapshot dates:\")\n",
    "for snapshot_date in snapshot_dates:\n",
    "    print(snapshot_date.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6cc5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_TO_RETAIN = ['building','amenity','leisure','public_transport','office','shop','tourism']\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"crs\", StructType([\n",
    "        StructField(\"properties\", StructType([\n",
    "            StructField(\"name\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"type\", StringType(), True)\n",
    "    ]), True),\n",
    "\n",
    "    StructField(\"features\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"geometry\", GeometryType(), True), \n",
    "            StructField(\"properties\", StructType([\n",
    "                StructField(\"amenity\", StringType(), True),\n",
    "                StructField(\"building\", StringType(), True),\n",
    "                StructField(\"element\", StringType(), True),\n",
    "                StructField(\"id\", LongType(), True),\n",
    "                StructField(\"leisure\", StringType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"office\", StringType(), True),\n",
    "                StructField(\"province\", StringType(), True),\n",
    "                StructField(\"public_transport\", StringType(), True),\n",
    "                StructField(\"region\", StringType(), True),\n",
    "                StructField(\"shop\", StringType(), True),\n",
    "                StructField(\"tourism\", StringType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"type\", StringType(), True)\n",
    "        ])\n",
    "    ), True),\n",
    "\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd44c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing snapshot dates:   0%|          | 0/30 [00:00<?, ?it/s]25/05/30 09:56:56 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=30600Kb max_used=30604Kb free=100472Kb\n",
      " bounds [0x000000010a9f8000, 0x000000010c808000, 0x00000001129f8000]\n",
      " total_blobs=11689 nmethods=10623 adapters=977\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:======================>                                 (20 + 8) / 49]\r"
     ]
    }
   ],
   "source": [
    "unique_features = {}\n",
    "for snapshot_date in tqdm(snapshot_dates, desc=\"Processing snapshot dates\"):\n",
    "    snapshot_date_str = snapshot_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    df = sedona.read.format(\"geojson\") \\\n",
    "        .option(\"multiLine\", \"true\") \\\n",
    "        .schema(schema) \\\n",
    "        .load(f\"s3a://amenities-dataset/amenities_v2/date={snapshot_date_str}/\")\n",
    "\n",
    "    exploded_df = df.select(explode(\"features\").alias(\"feature\")).repartition(8).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Select all relevant properties in one pass\n",
    "    values_df = exploded_df.select(\n",
    "        *[col(f\"feature.properties.{feature}\").alias(feature) for feature in FEATURES_TO_RETAIN]\n",
    "    )\n",
    "\n",
    "    for feature in FEATURES_TO_RETAIN:\n",
    "        distinct_values = (\n",
    "            values_df\n",
    "            .select(feature)\n",
    "            # .filter(col(feature).isNotNull())\n",
    "            .distinct()\n",
    "            .rdd.flatMap(lambda x: x)\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        unique_features.setdefault(feature, set()).update(distinct_values)\n",
    "\n",
    "    exploded_df.unpersist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aa9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "serializable_features = {k: list(v) for k, v in unique_features.items()}\n",
    "\n",
    "with open(\"unique_features.json\", \"w\") as f:\n",
    "    json.dump(serializable_features, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019d175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bcfb18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdcc-final-exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
