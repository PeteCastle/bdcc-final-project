{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, array, explode, struct, udf, count\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "import json\n",
    "from functools import reduce\n",
    "from datetime import datetime, date\n",
    "from pathlib import Path\n",
    "from sedona.spark import SedonaContext\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType, MapType\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.register import SedonaRegistrator\n",
    "from pyspark.sql.functions import input_file_name\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = Path('../_cache')\n",
    "LOGS_DIR = Path('../_logs')\n",
    "DATASET_DIR = Path('../datasets')\n",
    "OUTPUT_DIR = Path('../outputs')\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona = SedonaContext.builder() \\\n",
    "    .appName(\"BDCCFinalExam\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.storage.memoryFraction\",\"0.4\") \\\n",
    "    .config(\"spark.memory.fraction\",\"0.6\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.1,'\n",
    "            \"org.apache.hadoop:hadoop-aws:3.2.0,\"\\\n",
    "            'org.datasyslab:geotools-wrapper:1.7.1-28.5,'\\\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.11.375\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/qqxkptrd1bg66t_3104htv1m0000gn/T/ipykernel_14820/98655461.py:2: DeprecationWarning: Call to deprecated function registerAll (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  SedonaRegistrator.registerAll(sedona)\n",
      "25/06/08 21:58:56 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/06/08 21:58:56 WARN SimpleFunctionRegistry: The function rs_union_aggr replaced a previously registered function.\n",
      "25/06/08 21:58:56 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/06/08 21:58:56 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.common.geometryObjects.Geography, which is already registered.\n",
      "25/06/08 21:58:56 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/06/08 21:58:56 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/06/08 21:58:56 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n",
      "25/06/08 21:58:56 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register Sedona UDTs and functions\n",
    "SedonaRegistrator.registerAll(sedona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 ‚Äî Define `process_province()` function\n",
    "\n",
    "This function implements the full data processing pipeline for generating machine-learning-ready features from a single province's GeoJSON data.\n",
    "\n",
    "#### Inputs:\n",
    "- `df`: Spark DataFrame containing the `properties` of the province GeoJSON\n",
    "- `mapping_dict`: dictionary mapping raw feature values to clean categories\n",
    "- `quarter_date`: string representing the current quarter (e.g. \"2018-07-01\"), to be added as a column for time-based analysis\n",
    "\n",
    "#### Processing steps:\n",
    "1Ô∏è‚É£ Selects and handles relevant feature columns:\n",
    "   - `building`, `amenity`, `leisure`, `public_transport`, `office`, `shop`, `tourism`\n",
    "   - Handles flexible schemas ‚Äî only uses columns present in the file.\n",
    "\n",
    "2Ô∏è‚É£ Explodes multi-column categorical data into a long format:\n",
    "   - Ensures multiple categories per row are properly represented.\n",
    "\n",
    "3Ô∏è‚É£ Maps raw feature values to clean categories:\n",
    "   - Uses a **case-insensitive mapping**.\n",
    "   - Any unknown or unmapped values are assigned a safe default category `\"uncat__OTHER\"` to prevent pivot column collisions.\n",
    "\n",
    "4Ô∏è‚É£ Aggregates feature counts:\n",
    "   - Performs a groupby-pivot to create a wide feature table, with one column per category and counts as values.\n",
    "\n",
    "5Ô∏è‚É£ Adds metadata columns:\n",
    "   - Renames `\"province\"` ‚Üí `\"gadm\"` for consistency with administrative boundaries.\n",
    "   - Adds a `\"date\"` column with the quarter date, enabling time-based ML.\n",
    "\n",
    "#### Output:\n",
    "- Returns a Spark DataFrame with:\n",
    "  - One row per province\n",
    "  - Feature columns = category counts\n",
    "  - `\"gadm\"` column = province name\n",
    "  - `\"date\"` column = quarter date\n",
    "\n",
    "#### Purpose:\n",
    "- The resulting DataFrame is suitable for:\n",
    "  - ML feature engineering\n",
    "  - Time-series analysis\n",
    "  - Clustering and classification\n",
    "  - Longitudinal studies of urban change and amenities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def process_province(df, mapping_dict, quarter_date):\n",
    "    # 1Ô∏è‚É£ Safe default category\n",
    "    DEFAULT_CATEGORY = \"uncat__OTHER\"\n",
    "\n",
    "    # 2Ô∏è‚É£ Define type columns\n",
    "    desired_type_columns = [\n",
    "        \"building\",\n",
    "        \"amenity\",\n",
    "        \"leisure\",\n",
    "        \"public_transport\",\n",
    "        \"office\",\n",
    "        \"shop\",\n",
    "        \"tourism\"\n",
    "    ]\n",
    "    type_columns = [col for col in desired_type_columns if col in df.columns]\n",
    "\n",
    "    # 3Ô∏è‚É£ Explode columns\n",
    "    exploded_array = array(\n",
    "        *[struct(lit(c).alias(\"column\"), col(c).alias(\"value\")) for c in type_columns]\n",
    "    )\n",
    "    df_exploded = (\n",
    "        df\n",
    "        .withColumn(\"exploded\", exploded_array)\n",
    "        .withColumn(\"exploded\", explode(col(\"exploded\")))\n",
    "        .select(\n",
    "            \"province\",  # will rename later\n",
    "            \"exploded.column\",\n",
    "            \"exploded.value\"\n",
    "        )\n",
    "    )\n",
    "    df_exploded = df_exploded.filter(col(\"value\").isNotNull())\n",
    "\n",
    "    # 4Ô∏è‚É£ Define UDF ‚Äî case-insensitive mapping with safe default\n",
    "    def map_to_category(value):\n",
    "        if value is None:\n",
    "            return DEFAULT_CATEGORY\n",
    "        return mapping_dict.get(value.lower(), DEFAULT_CATEGORY)\n",
    "\n",
    "    map_to_category_udf = udf(map_to_category, StringType())\n",
    "\n",
    "    # 5Ô∏è‚É£ Map to new_category\n",
    "    df_mapped = df_exploded.withColumn(\n",
    "        \"new_category\",\n",
    "        map_to_category_udf(col(\"value\"))\n",
    "    )\n",
    "\n",
    "    # 6Ô∏è‚É£ Pivot ‚Äî aggregate counts\n",
    "    df_final = (\n",
    "        df_mapped\n",
    "        .groupBy(\"province\")  # will rename later\n",
    "        .pivot(\"new_category\")\n",
    "        .agg(count(\"value\"))\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    # 7Ô∏è‚É£ Rename province ‚Üí gadm\n",
    "    df_final = df_final.withColumnRenamed(\"province\", \"gadm\")\n",
    "\n",
    "    # 8Ô∏è‚É£ Add date column\n",
    "    df_final = df_final.withColumn(\"date\", lit(quarter_date))\n",
    "\n",
    "    # 9Ô∏è‚É£ Return final DataFrame\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 ‚Äî Per-quarter processing: generate single Parquet file\n",
    "\n",
    "This block processes **one quarter (date folder)** of GeoJSON data:\n",
    "\n",
    "#### Inputs:\n",
    "- `quarter_date`: quarter to process (ex: \"2018-07-01\")\n",
    "- `quarter_folder`: path to `date=YYYY-MM-DD` folder\n",
    "\n",
    "#### Processing steps:\n",
    "1Ô∏è‚É£ Loads the category mapping:\n",
    "   - Ensures all mappings are **case-insensitive**.\n",
    "   - Prevents `\"uncategorized\"` from being used as a mapped category ‚Äî uses `\"uncat__OTHER\"` instead.\n",
    "\n",
    "2Ô∏è‚É£ Iterates through:\n",
    "   - Each region folder in this quarter.\n",
    "   - Each province GeoJSON file inside the region folder.\n",
    "\n",
    "3Ô∏è‚É£ For each province:\n",
    "   - Loads and flattens GeoJSON ‚Üí Spark DataFrame.\n",
    "   - Drops any conflicting `\"uncategorized\"` column if present in raw data.\n",
    "   - Runs `process_province()` to produce an ML feature table for that province.\n",
    "   - Collects each province feature table into `df_list`.\n",
    "\n",
    "4Ô∏è‚É£ Combines all provinces for this quarter:\n",
    "   - Uses `unionByName(allowMissingColumns=True)` to align columns across provinces (some categories may not appear in every province).\n",
    "   - Result is one unified DataFrame for the entire quarter.\n",
    "\n",
    "5Ô∏è‚É£ Saves result:\n",
    "   - Saves one **Parquet dataset per quarter** ‚Üí ready for ML pipelines.\n",
    "   - Output path: `output/features_quarter=<quarter_date>/features.parquet`\n",
    "\n",
    "#### Output:\n",
    "- A single **Parquet file** per quarter, containing one row per province with:\n",
    "  - `\"gadm\"` = province name\n",
    "  - Category count columns (features)\n",
    "  - `\"date\"` column\n",
    "\n",
    "#### Purpose:\n",
    "- Standardizes feature engineering across all quarters.\n",
    "- Ensures data is ready for:\n",
    "  - Longitudinal time-based ML\n",
    "  - Clustering\n",
    "  - Classification\n",
    "  - Visualizations\n",
    "  - Reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Transform Amenity Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"crs\", StructType([\n",
    "        StructField(\"properties\", StructType([\n",
    "            StructField(\"name\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"type\", StringType(), True)\n",
    "    ]), True),\n",
    "\n",
    "    StructField(\"features\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"geometry\", GeometryType(), True), \n",
    "            StructField(\"properties\", StructType([\n",
    "                StructField(\"amenity\", StringType(), True),\n",
    "                StructField(\"building\", StringType(), True),\n",
    "                StructField(\"element\", StringType(), True),\n",
    "                StructField(\"id\", LongType(), True),\n",
    "                StructField(\"leisure\", StringType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"office\", StringType(), True),\n",
    "                StructField(\"province\", StringType(), True),\n",
    "                StructField(\"public_transport\", StringType(), True),\n",
    "                StructField(\"region\", StringType(), True),\n",
    "                StructField(\"shop\", StringType(), True),\n",
    "                StructField(\"tourism\", StringType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"type\", StringType(), True)\n",
    "        ])\n",
    "    ), True),\n",
    "\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot dates:\n",
      "[datetime.date(2023, 1, 1), datetime.date(2023, 4, 1), datetime.date(2023, 7, 1), datetime.date(2023, 10, 1), datetime.date(2022, 1, 1), datetime.date(2022, 4, 1), datetime.date(2022, 7, 1), datetime.date(2022, 10, 1), datetime.date(2021, 1, 1), datetime.date(2021, 4, 1), datetime.date(2021, 7, 1), datetime.date(2021, 10, 1), datetime.date(2020, 1, 1), datetime.date(2020, 4, 1), datetime.date(2020, 7, 1), datetime.date(2020, 10, 1), datetime.date(2019, 1, 1), datetime.date(2019, 4, 1), datetime.date(2019, 7, 1), datetime.date(2019, 10, 1), datetime.date(2018, 1, 1), datetime.date(2018, 4, 1), datetime.date(2018, 7, 1), datetime.date(2018, 10, 1)]\n"
     ]
    }
   ],
   "source": [
    "snapshot_dates = [\n",
    "    date(year, month, 1)\n",
    "    for year in range(2023, 2017, -1)\n",
    "    for month in (1, 4, 7, 10)  # January, April, July, October\n",
    "]\n",
    "\n",
    "print(\"Snapshot dates:\")\n",
    "print(snapshot_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CATEGORY = \"uncat__OTHER\"\n",
    "\n",
    "with open(DATASET_DIR / \"feature_to_new_category.json\", \"r\") as f:\n",
    "    mapping_dict_raw = json.load(f)\n",
    "\n",
    "mapping_dict = {\n",
    "    k.lower(): (v if v.lower() != \"uncategorized\" else DEFAULT_CATEGORY)\n",
    "    for k, v in mapping_dict_raw.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in snapshot_dates:\n",
    "    date__str = date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    df_raw = sedona.read.format(\"geojson\") \\\n",
    "        .option(\"multiLine\", \"true\") \\\n",
    "        .schema(schema) \\\n",
    "        .load(f\"s3a://amenities-dataset/amenities_v2/date={date__str}\")\n",
    "\n",
    "    df_exploded = df_raw.select(explode(\"features\").alias(\"feature\"))\n",
    "    df = df_exploded.select(\"feature.properties.*\")\n",
    "\n",
    "    # üöÄ Drop conflicting column if exists\n",
    "    if \"uncategorized\" in df.columns:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Dropping conflicting column 'uncategorized' in {file_path}\")\n",
    "        df = df.drop(\"uncategorized\")\n",
    "\n",
    "    df_final = process_province(df, mapping_dict, date__str)\n",
    "    df_final.write.mode(\"overwrite\").parquet(str(OUTPUT_DIR / \"features\" / f\"features_quarter={date__str}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdcc-final-exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
